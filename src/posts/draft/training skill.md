---
title: 'Training skill'
date: '2020-06-22'
category: []
draft: True
---

ì´ë²ˆì¥ì—ì„œ ì•Œì•„ë³¼ Training Skillì€ í¬ê²Œ ë‹¤ìŒê³¼ ê°™ë‹¤.

-   **_ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ìµœì í™”_** ë°©ë²•,
-   **_ë§¤ê°œë³€ìˆ˜ ì´ˆê¹ƒê°’, í•˜ì´í¼íŒŒë¼ë¯¸í„°(lr ë“±) ì„¤ì •_**ë°©ë²•
-   **_overfittingì˜ ëŒ€ì‘ì±…ì¸ ê°€ì¤‘ì¹˜ ê°ì†Œì™€ ë“œë¡­ì•„ì›ƒ_**(ì •ê·œí™”ë°©ë²•)
-   **_ë°°ì¹˜ ì •ê·œí™”_**

---

### ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 

ì§€ê¸ˆê¹Œì§€ëŠ” ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’ì„ `SGD(í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•)`ì„ í†µí•´ì„œ ì°¾ì•„ë‚˜ê°”ë‹¤.
ì´ëŸ° SGDì˜ ë‹¨ì ì„ ì•Œì•„ë³´ê³  ë‹¤ë¥¸ ìµœì í™”ê¸°ë²•ì„ ì‚¬ìš©í•´ë³´ì

#### SGD

`W = W - lr*gradient` ë¡œ í‘œí˜„í• ìˆ˜ ìˆë‹¤.**_ ì§ê´€ì ìœ¼ë¡œ ë³´ë©´ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì •ê±°ë¦¬ ê°€ê² ë‹¤ëŠ” ë°©ë²•_**.

ì˜ì‚¬ì½”ë“œë¡œ `SGD`ë¥¼ ì‚¬ìš©í•˜ëŠ” ë²•ì„ ì‘ì„±í•´ë³´ë©´

```python
network = TwoLayerNet()
optimizer = SGD()

for i in range(10000):
    ...
    x_batch, t_batch = get_mini_batch(...) #ë¯¸ë‹ˆë°°ì¹˜
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads) #ì—¬ê¸°ì„œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ì„ í•´ì¤€ë‹¤.
```

ì´ì²˜ëŸ¼ ëŒ€ë¶€ë¶„ì˜ DL í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ìµœì í™”ê¸°ë²•ê³¼ ê°™ì€ê²ƒì„ **_ëª¨ë“ˆí™”_** ì‹œì¼œë†“ê³  ì‚¬ìš©í•˜ê¸° í¸í•˜ê²Œ í•´ì¤€ë‹¤(ex. Lasagne framework)

---

#### SGD

SGDëŠ” ë‹¨ìˆœí•˜ê³  êµ¬í˜„í•˜ê¸° ì‰½ì§€ë§Œ ë¬¸ì œì— ë”°ë¼ì„œëŠ” ë¹„íš¨ìœ¨ì ì´ë‹¤. íŠ¹íˆ `anisotropy function`(ë°©í–¥ì—ë”°ë¼ ê¸°ìš¸ê¸°ê°€ ë‹¬ë¼ì§€ëŠ” í•¨ìˆ˜)ì—ì„œëŠ” íƒìƒ‰ ê²½ë¡œê°€ ë¹„íš¨ìœ¨ì ì´ê²Œ ë‚˜ì˜¬ìˆ˜ìˆë‹¤.

---

### Momentum (ëª¨ë©˜í…€)

ìš´ë™ëŸ‰ì„ ëœ»í•˜ëŠ” ë§Œí¼ ë¬¼ë¦¬ì™€ ê´€ê³„ê°€ ìˆëŠ”ë°

`v = av - lr*gradient` (avëŠ” ì•„ë¬´ í˜ì´ ì—†ì„ë•Œ í•˜ê°•ì„ ì˜ë¯¸í•œë‹¤ ì¤‘ë ¥ê°€ì†ë„ë“±)
`W = W + v` ì¦‰ ê¸°ìš¸ê¸°ë°©í–¥ìœ¼ë¡œ **_ê°€ì†_**ì´ ë¶™ëŠ”ë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í•œ ì‹ì´ë‹¤.

ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´ **_ê³µì´ ë°”ë‹¥ì„ êµ¬ë¥´ë“¯_** ì›€ì§ì´ê¸°ë•Œë¬¸ì— SGDì— ë¹„í•´ì„œ ê²½ë¡œê°€ íš¨ìœ¨ì ì´ë‹¤.(**_ê¸°ìš¸ê¸°ì˜ ì˜í–¥ì„ ì ê²Œë°›ëŠ” ì¶•ìœ¼ë¡œ ê³ ì •ëœ í˜ì„ ë°›ê³ ìˆë‹¤ê³  ê°€ì •_**í•˜ê¸°ë•Œë¬¸ì—)

### AdaGrad

ì‹ ê²½ë§í•™ìŠµì—ì„œëŠ” **_í•™ìŠµë¥ (lr)_** ê°’ì´ ì¤‘ìš”í•˜ë‹¤. ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì‹œê°„ì´ ê¸¸ì–´ì§€ê³ , ë„ˆë¬´ í¬ë©´ ë°œì‚°ìœ¼ë¡œ íŠ•ê²¨ì € ë‚˜ê°„ë‹¤

ì´ í•™ìŠµë¥ ì„ ì •í•˜ëŠ” ê¸°ìˆ ë¡œëŠ” `learning rate decay` ê°€ ìˆëŠ”ë° í•™ìŠµì„ ì§„í–‰í•˜ë©´ì„œ ì ì°¨ ì¤„ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤. ì´ë•Œ ë§¤ê°œë³€ìˆ˜ ì „ì²´ì˜ í•™ìŠµë¥ ì„ ì¼ê´„ì ìœ¼ë¡œ ë‚®ì¶”ëŠ” ë°©ë²•ì´ ìˆëŠ”ë° ì´ë¥¼ ë°œì „ì‹œí‚¨ê²Œ `AdaGrad(ê°ê°ì˜ ë§¤ê°œë³€ìˆ˜ì— ë§ì¶¤í˜• ê°’ì„ ë§Œë“ ë‹¤)`

```python
h = h+gradient**2
W = W - lr*1/n^1/2*gradient
```

ì´ì²˜ëŸ¼ **_hëŠ” ê¸°ìš¸ê¸°ê°’ì„ ì œê³±í•´ì„œ ê³„ì† ë”í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í• ë•Œ ê¸°ìš¸ê¸°ê°€ í° ì›ì†Œ(ë§ì´ ì›€ì§ì¸)ì— ëŒ€í•´ì„œëŠ” í•™ìŠµë¥ ì„ ë‚®ê²Œ ì ìš©_**í•œë‹¤. ì¦‰ ë§¤ê°œë³€ìˆ˜ ì›ì†Œë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì ìš©í•œë‹¤

ê·¸ë¦¬ê³  **_ê³„ì† ë”í•´ê°€ëŠ”ë§Œí¼ í•™ìŠµì„ ì§„í–‰í• ìˆ˜ë¡ ê°±ì‹  ê°•ë„ê°€ ì•½í•´ì§„ë‹¤_**.
ê·¸ë˜ì„œ ì´ë¥¼ ê°œì„ í•œ ê¸°ë²•ì´ `RMSPRop`ë¼ëŠ” ë°©ë²•ì´ ìˆë‹¤. ì´ ë°©ë²•ì€ ê³¼ê±°ì˜ ê¸°ìš¸ê¸°ëŠ” ì ì  ìŠê³  ìƒˆë¡œìš´ ê¸°ìš¸ê¸°ë¥¼ í¬ê²Œ ë°˜ì˜í•˜ëŠ”ë°, ì´ë¥¼ **ì§€ìˆ˜ì´ë™í‰ê· **ì´ë¼ í•˜ë©° ê³¼ê±° ê¸°ìš¸ê¸°ì˜ ë¹„ì¤‘ì„ ë‚®ì¶° ì£¼ëŠ” ê²ƒì´ë‹¤.

ë”°ë¼ì„œ ê°±ì‹  ê²½ë¡œë¥¼ í™•ì¸í•´ë³´ë©´ ì´ë™ì´ í° **yì¶• ë°©í–¥ìœ¼ë¡œëŠ” ì´í›„ë¶€í„° ì†ë„ë¥¼ ì¤„ì—¬ì„œ ì´ë™**í•˜ê¸° ì‹œì‘í•˜ê³  xëŠ” ê·¸ì— ë¹„í•´ ì˜í–¥ì„ ëœ ë°›ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤.

### Adam (Momentum + AdaGrad)

ëª¨ë©˜í…€ì²˜ëŸ¼ ë¬¼ë¦¬ì ì¸ ë²•ì¹™ì„ ì´ìš©í•˜ëŠ” ê²ƒê³¼ ë”ë¶ˆì–´ì„œ ë§¤ê°œë³€ìˆ˜ì˜ ì›ì†Œë§ˆë‹¤ ì ì‘ì ìœ¼ë¡œ ê°±ì‹ ì •ë„ë¥¼ ì¡°ì •í•˜ëŠ” AdaGrad ë¥¼ ìœµí•©í•´ì„œ ë§Œë“¤ì—ˆë‹¤.
2015ë…„ ì œì•ˆë˜ì—ˆëŠ”ë° ë‘ë°©ë²•ì„ ì¡°í•©í•œë§Œí¼ íš¨ìœ¨ì ì¸ íƒìƒ‰ê³¼ ë”ë¶ˆì–´ **_í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ í¸í–¥ ë³´ì •ì´ ì§„í–‰_**ëœë‹¤.

ì´ì²˜ëŸ¼ `Adam`ë„ ê·¸ë¦‡ë°”ë‹¥ì„ êµ¬ë¥´ë“¯ ì›€ì§ì´ëŠ”ë°, `Momentum`ë³´ë‹¤ ì§€ê·¸ì¬ê·¸ë¡œ ì›€ì§ì´ëŠ”ê²Œ ì ë‹¤.`AdaGrad`ë¥¼ ì ìš©í•œ íš¨ê³¼ì´ë‹¤.

> Adamì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ 3ê°œ ì„¤ì •í•˜ëŠ”ë° í•˜ë‚˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ í•™ìŠµë¥ ,(lr), ë‚˜ë¨¸ì§€ ë‘ê°œëŠ” ì¼ì°¨ ëª¨ë©˜í…€ìš© ê³„ìˆ˜(0.9)ì™€ ì´ì°¨ ëª¨ë©˜í…€ìš© ê³„ìˆ˜(0.999)ë¥¼ ëœ»í•œë‹¤.

ìœ„ì˜ 4ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ ì–´ë–¤ ë¬¸ì œì¸ì§€ì— ë”°ë¼ì„œ, í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠ”ê°€ì— ë”°ë¼ì„œë„ ë‹¬ë¼ì§„ë‹¤.(ì‹ ê²½ë§ì˜ êµ¬ì¡°, ì´ ê¹Šì´ì— ë”°ë¼ì„œë„)

ì§€ê¸ˆìœ¼ë¡œì¨ëŠ” `SGD` ì™€ `Adam`ì„ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤.
ê·¸ì¹˜ë§Œ **_ë” ì¼ë°˜ì ìœ¼ë¡œ_** SGDë³´ë‹¤ `Adam`ì´ ë” ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³  ìµœì¢… ì •í™•ë„ë„ ë†’ê²Œ ë‚˜íƒ€ë‚œë‹¤.

### ê°€ì¤‘ì¹˜ì˜ ì´ˆê¹ƒê°’

ê°€ì¤‘ì¹˜ì˜ ì´ˆê¹ƒê°’ì„ ë¬´ì—‡ìœ¼ë¡œ ì„¤ì •í•˜ëŠëƒì— ë”°ë¼ í•™ìŠµì˜ ì„±íŒ¨ë¥¼ ê°€ë¥´ëŠ” ì¼ì´ ë§ë‹¤.

-   `overfitting`ì„ ì–µì œí•´ ë²”ìš© ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ì¸ **ê°€ì¤‘ì¹˜ ê°ì†Œ**
    ê°€ì¤‘ì¹˜ ê°’ì„ ì‘ê²Œí•´ì„œ ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒ.

ë§Œì•½ ê·¸ëŸ¼ ì´ˆê¹ƒê°’ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ? => í•™ìŠµì´ ì˜¬ë°”ë¡œ ì´ë¤„ì§€ì§€ ì•ŠëŠ”ë‹¤.

> â” ì´ˆê¹ƒê°’ì„ ëª¨ë‘ 0ìœ¼ë¡œ í•´ì„œ ì•ˆë˜ëŠ” ì´ìœ ëŠ”?
> (ì •í™•íˆëŠ” ê°€ì¤‘ì¹˜ë¥¼ ê· ì¼ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì•ˆë˜ëŠ” ì´ìœ ) ì˜¤ì°¨ì—­ì „íŒŒë²•ì—ì„œ ëª¨ë“  ê°€ì¤‘ì¹˜ì˜ ê°’ì´ ë˜‘ê°™ì´ ê°±ì‹ ë˜ê¸°ë•Œë¬¸ì´ë‹¤.  
> ì¦‰ ìˆœì „íŒŒë¡œ ê°ˆë•Œ ë‘ë²ˆì§¸ì¸µì˜ ë‰´ëŸ°ì— ê°™ì€ ê°’ì´ ê°€ëŠ”ë° ì´ëŠ” ì—­ì „íŒŒë¡œ ëŒì•„ì˜¬ë•Œë„ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ ë˜‘ê°™ì´ ê°±ì‹ ëœë‹¤ëŠ” ë§ì´ë‹¤. => ì¦‰ ê°±ì‹ ì„ ê±°ì³ë„ ê°™ì€ ê°’ì„ ìœ ì§€í•œë‹¤.(ë”°ë¼ì„œ ì´ˆê¹ƒê°’ì€ ëŒ€ì¹­ì ì¸ ê· í˜•ì„ ë¬´ë„ˆëœ¨ë¦¬ë©´ì„œ ë¬´ì‘ìœ„ë¡œ ì„¤ì •í•´ì¤˜ì•¼í•œë‹¤.)

-   ì€ë‹‰ì¸µì˜ **_í™œì„±í™”ê°’(í™œì„±í™”í•¨ìˆ˜ì˜ ì¶œë ¥ë°ì´í„°)_**ì˜ ë¶„í¬ë¥¼ ê´€ì°°í•´ë³´ì
    í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” 5ì¸µ ì‹ ê²½ë§ì— ë¬´ì‘ìœ„ë¡œ ìƒì„±í•œ input dataë¥¼ ë„£ì–´ ê° ì¸µì˜ í™œì„±í™” ê°’ì˜ ë¶„í¬ë¥¼ í™•ì¸í•´ë³´ì

ì¼ë°˜ì ì¸ `sigmoid`ë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ í•˜ê²Œë˜ë©´(í‘œì¤€í¸ì°¨ 1) í™œì„±í™” ê°’ë“¤ì´ 0ê³¼ 1ì— ì¹˜ìš°ì³ ë¶„í¬ê°€ ë˜ì–´ìˆê¸°ë•Œë¬¸ì— ì—­ì „íŒŒì˜ ê¸°ìš¸ê¸° ê°’ì´ ì ì  ì‘ì•„ì§€ë‹¤ê°€ ì‚¬ë¼ì§€ëŠ”
`gradient vanishing`ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í™œì„±í™”ê°’ë“¤ì´ íŠ¹ì • ê°’ì— ì¹˜ìš°ì¹œê²Œ ì•„ë‹Œ ê³ ë¥´ê³  ë‹¤ì–‘í•˜ê²Œ ë¶„í¬í•´ì•¼ì§€ ì˜¬ë°”ë¥¸ í•™ìŠµì„ í• ìˆ˜ ìˆë‹¤. ì¹˜ìš°ì³ì„œ ê°™ì€ ê°’ì„ ì¶œë ¥í•œë‹¤ëŠ” ê²ƒì€ ë‰´ëŸ°ì´ **_ë§ì•„ë„ ê°™ì€ ê°’ì„ ì¶œë ¥í•œë‹¤ëŠ” ê²ƒì´ë¯€ë¡œ ì˜ë¯¸ê°€ ì—†ë‹¤_**.(**_í‘œí˜„ë ¥ì´ ì œí•œëœë‹¤_**)

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ì„
`Xavier` ì´ˆê¹ƒê°’ì„ ì´ìš©í•´ì„œ ì„¤ì •í•´ë³´ì
(`Xavier` ì´ˆê¹ƒê°’ì€ ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ í‘œì¤€ì ìœ¼ë¡œ ì´ìš©í•˜ê³  ìˆë‹¤.)
(Caffe í”„ë ˆì„ì›Œí¬ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ì„ ì„¤ì •í• ë–„ args ë¡œ xavierë¡œ ì§€ì •í• ìˆ˜ìˆë‹¤.)

ì£¼ëœ ëª©ì ì€ **ê° ì¸µì˜ í™œì„±í™”ê°’ì„ ê´‘ë²”ìœ„í•˜ê²Œ ë¶„í¬**ì‹œí‚¤ëŠ”ê²ƒì´ê³  ì• ê³„ì¸µì˜ ë…¸ë“œê°€ nê°œì´ë©´ **_í‘œì¤€í¸ì°¨ë¥¼ 1/n^1/2ë¡œ ì‚¬ìš©_**í•˜ë©´ ëœë‹¤ëŠ” ê²°ë¡ ì´ ë‚˜ì˜¨ë‹¤
(ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒì¸µì˜ ë…¸ë“œë„ ê³ ë ¤í–ˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ìˆœí™”í•´ì„œ ì‚¬ìš©í•œë‹¤.)

> ë‹¨ ì´ë•Œ **_Xavier ì´ˆê¹ƒê°’ì€ í™œì„±í™” í•¨ìˆ˜ê°€ ì„ í˜•_**ì¸ê²ƒì„ ì „ì œë¡œ í•œë‹¤(**_sigmoid ê³¼ tanhëŠ” ì¢Œìš° ëŒ€ì¹­ì´ë¼ ì¤‘ì•™ ë¶€ê·¼ì—ì„œ ì„ í˜•_**ì´ë‹¤.)

### ReLUë¥¼ ì‚¬ìš©í• ë•Œ ì´ˆê¹ƒê°’

ReLUë¥¼ ì´ìš©í• ë•Œì˜ ì´ˆê¹ƒê°’ì„ `He` ì´ˆê¹ƒê°’ì´ë¼ í•œë‹¤.
ê·¸ë¦¬ê³  ì´ˆê¹ƒê°’ì€ ì•ì¸µì˜ ë…¸ë“œê°€ `n`ì¼ ë•Œ `(2/n)^1/2` ë¡œ ì£¼ì–´ì§„ë‹¤.
(ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´ **ReLUëŠ” 0ì´í•˜ê°€ 0ì´ë¯€ë¡œ ì´ì „ì˜ sigmoidì— ë¹„í•´ì„œ 2ë°°ì˜ ê°’ì´ í•„ìš”í•˜ë‹¤**)
ì´ë¥¼ í†µí•´ì„œ ê°€ì¤‘ì¹˜ ê°’ì„ ì´ˆê¸°í™”í•˜ë©´ ì¸µì´ ê¹Šì–´ì ¸ë„ í™œì„±í™” ê°’ë“¤ì´ ì¹˜ìš°ì¹˜ì§€ ì•Šê³ 
**_ê¸°ìš¸ê¸° ì†Œì‹¤_** ë¬¸ì œë„ ì—†ì–´ì§„ë‹¤.(ë”°ë¼ì„œ ì—­ì „íŒŒë•Œë„ ì ì ˆí•œ ê°’ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤.)

ë”°ë¼ì„œ **_ì´ˆê¹ƒê°’ì˜ ì„¤ì •ì€ ì •ë§ ì¤‘ìš”í•œ ë¬¸ì œ_**ì´ë‹¤.(ì‹ ê²½ë§ í•™ìŠµì˜ ì„±íŒ¨ë¥¼ ê²°ì •ì§“ëŠ” ê²½ìš°ë„ ë§ë‹¤.)

### ë°°ì¹˜ ì •ê·œí™”

ìœ„ì˜ ë°©ë²•ë“¤ì€ ê°€ì¤‘ì¹˜ì˜ ì´ˆê¹ƒê°’ì„ ì˜ ì„¤ì •í•˜ì—¬ í™œì„±í™” ê°’ì„ í¼ëœ¨ë¦° ë°˜ë©´
ì´ë²ˆì—ëŠ” **_ê°•ì œë¡œ í™œì„±í™” ê°’ì„ í¼ëœ¨ë ¤_** ë³´ë„ë¡ í•˜ì.

-   **_í•™ìŠµì„ ë¹¨ë¦¬ ì§„í–‰_**í• ìˆ˜ìˆë‹¤.
-   **_ì´ˆê¹ƒê°’ì— í¬ê²Œ ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤_**
-   **_ì˜¤ë²„í”¼íŒ…ì„ ì–µì œ_**í•œë‹¤

input data --> Affine --> `Batch Norm` --> ReLU --> Affine --> `Batch Norm` --> ReLU --> Affine --> Softmax

ë°°ì¹˜ì •ê·œí™”ëŠ” ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë‹¨ìœ„ë¡œ ì •ê·œí™” í•´ì¤€ë‹¤.(í‰ê· ì´ 0, ë¶„ì‚°ì´ 1)
ê·¸ë¦¬ê³  ì •ê·œí™”ëœ dataì— í™•ëŒ€ì™€ ì´ë™ì„ í•´ì¤„ìˆ˜ìˆëŠ”ë°
`y = Î¥x + Î²`
**_Î¥ê°€ í™•ëŒ€_** **_Î²ê°€ ì´ë™_**ì„ ë‹´ë‹¹í•œë‹¤.(ê·¸ëƒ¥ 1ì°¨ì‹ ì´ë™ê³¼ ê¸°ìš¸ê¸° ë³€í™”ì™€ ê°™ì€ ê°œë…ì´ë‹¤.)
~~(ë°°ì¹˜ ì •ê·œí™”ì˜ ì—­ì „íŒŒëŠ” ê¶ê¸ˆí•˜ë©´ í”„ë ˆë“œë¦­ í¬ë ˆì €íŠ¸ì˜ ë¸”ë¡œê·¸ë¥¼ ì°¾ì•„ë³´ì)~~

**_ë°°ì¹˜ì •ê·œí™”_**ë¥¼ ì‹œí‚¤ë©´ ê±°ì˜ ëª¨ë“  ê²½ìš°ì—ì„œ í•™ìŠµì†ë„ê°€ ë¹¨ë¼ì§€ëŠ”ë°(ë¬´ì¡°ê±´ì€ ì•„ë‹ˆë‹¤) ì´ëŸ° ì†ë„ ë¿ë§Œì•„ë‹ˆë¼ ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ì— í¬ê²Œ ì˜ì¡´í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.

### ë°”ë¥¸ í•™ìŠµì„ ìœ„í•´

-   `overfitting` : ì‹ ê²½ë§ì´ `training data`ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ì ì‘í•˜ì—¬ ê·¸ ì´ì™¸ì˜ dataì—ëŠ” ì œëŒ€ë¡œ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ìƒíƒœ

overfittingì˜ ì›ì¸

-   ë§¤ê°œë³€ìˆ˜ê°€ ë§ê³  í‘œí˜„ë ¥ì´ ë†’ì€ ëª¨ë¸
-   training dataê°€ ì ì€ ëª¨ë¸

#### í•´ê²°ë°©ë²•

-   `ê°€ì¤‘ì¹˜ ê°ì†Œ`
    í° ê°€ì¤‘ì¹˜ì— ëŒ€í•´ì„œ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•´ `overfitting`ì„ ì–µì œí•˜ëŠ” ë°©ë²•
    (ì˜¤ë²„í”¼íŒ…ì€ ê°€ì¤‘ì¹˜ì˜ ë§¤ê°œë³€ìˆ˜ê°€ ì»¤ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë§ê¸°ë•Œë¬¸)
    ê°€ì¤‘ì¹˜ì˜ `L2 norm(ê°€ì¤‘ì¹˜ì˜ ì œê³± norm)`ì„ ì†ì‹¤í•¨ìˆ˜ì— ë”í•œë‹¤.
    ê°€ì¤‘ì¹˜ë¥¼ Wë¼ í•˜ë©´ `L2 norm`ì—ë”°ë¥¸ ê°€ì¤‘ì¹˜ ê°ì†ŒëŠ” `1/2Î»W^2`ê°€ ë˜ê³  ì´ë¥¼ ì†ì‹¤í•¨ìˆ˜ì— ë”í•œë‹¤

ê°€ì¤‘ì¹˜ê°ì†ŒëŠ” ëª¨ë“  ê°€ì¤‘ì¹˜ ê°ê°ì˜ ì†ì‹¤í•¨ìˆ˜ì— `1/2Î»W^2`ë¥¼ ë”í•˜ëŠ”ë° ì—­ì „íŒŒì— ë”°ë¼ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í• ë•ŒëŠ” ë¯¸ë¶„í•œ `Î»W`ë¥¼ ë”í•œë‹¤

> normì˜ ì¢…ë¥˜ì—ëŠ” `L2 norm`, `L1 norm`, `Lâˆ` ê°€ ìˆë‹¤.

-   `DropOut`  
    ì‹ ê²½ë§ ëª¨ë¸ì´ ë³µì¡í•´ì§€ë©´ ê°€ì¤‘ì¹˜ ê°ì†Œë§Œìœ¼ë¡œëŠ” ëŒ€ì‘í•˜ê¸° ì–´ë ¤ì›Œ ì§€ëŠ”ë°, ì´ëŸ´ë•ŒëŠ” `Dropout`ì´ë¼ëŠ” ê¸°ë²•ì„ ì´ìš©í•œë‹¤.

    Dropoutì€ **_ë‰´ëŸ°ì„ ì„ì˜ë¡œ ì‚­ì œí•˜ë©´ì„œ í•™ìŠµ_**í•˜ëŠ” ë°©ë²•
    **_í›ˆë ¨ë•ŒëŠ” dataë¥¼ í˜ë¦´ë•Œë§ˆë‹¤ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì‚­ì œí•˜ê³  ì‹œí—˜í• ë•ŒëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚´ë ¤ì„œ ì „ë‹¬_**í•œë‹¤. Dropoutì˜ íš¨ìœ¨ì ì¸ êµ¬í˜„ì´ ê¶ê¸ˆí•˜ë©´
    Chainer í”„ë ˆì„ì›Œí¬ì˜ ë“œë¡­ì•„ì›ƒ êµ¬í˜„ì„ ì°¸ê³ í•´ë³´ì

ë“œë¡­ì•„ì›ƒì„ ì´ìš©í•˜ë©´ **_í‘œí˜„ë ¥ì´ ë†’ì•„ì§€ë©´ì„œë„ overfitting_**ì„ ì–µì œí•  ìˆ˜ë„ ìˆë‹¤.

> MLì—ì„œëŠ” `ì•™ìƒë¸”`ì„ **_ì• ìš©í•˜ëŠ”ë° ê°œë³„ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê· ë‚´ì–´ ì¶”ë¡ í•˜ëŠ” ë°©ì‹_**ì´ë‹¤.
> ì´ëŠ” ë“œë¡­ì•„ì›ƒê³¼ë„ ë¹„ìŠ·í•˜ë‹¤ê³  ë³¼ìˆ˜ìˆëŠ”ë°, **_ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ë§¤ë²ˆ ì‚­ì œí•˜ëŠ” ê²ƒì„ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ë¼ê³  ìƒê°í•´ì£¼ë©´ ì•™ìƒë¸”ê³¼ ê°™ì€ ë°©ì‹_**ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.

### ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°

-   `ê²€ì¦ë°ì´í„°`
    **í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê²€ì¦í•  ë•ŒëŠ” Testdataë¥¼ ì‚¬ìš©í•´ì„œëŠ” ì•ˆëœë‹¤.**
    ==> ì™œëƒí•˜ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ Test dataì— ì˜¤ë²„í”¼íŒ… ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
    ë”°ë¼ì„œ ì „ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§ˆë ¨í•´ë†”ì•¼í•˜ëŠ”ë° ì´ë¥¼ ê²€ì¦ë°ì´í„°(**_validation data_**)ë¼ê³  ë¶€ë¥¸ë‹¤
    (ì¼ë°˜ì ìœ¼ë¡œëŠ” Train dataì˜ **_20%_**ì •ë„ë¥¼ ê²€ì¦ë°ì´í„°ë¡œ ë¹¼ì„œ ê²€ì¦í•œë‹¤.)

*   `ìµœì í™” ê³¼ì •`
    í•µì‹¬ì€ **ìµœì ê°’ì´ ì¡´ì¬í•˜ëŠ” ë²”ìœ„ë¥¼ ì¡°ê¸ˆì”© ì¤„ì—¬ë‚˜ê°€ëŠ” ê²ƒ**ì´ë‹¤.
    ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ê·¸ ë²”ìœ„ì—ì„œ **ë¬´ì‘ìœ„ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ê³¨ë¼ë‚¸(ìƒ˜í”Œë§) í›„ ê·¸ ê°’ìœ¼ë¡œ ì •í™•ë„ë¥¼ í‰ê°€**í•œë‹¤.

ì´ ì‘ì—…ì—ì„œëŠ” ê·¸ë¦¬ë“œ ì„œì¹˜ì™€ ê°™ì€ ê·œì¹™ì ì¸ íƒìƒ‰ë³´ë‹¤ **ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•´ì„œ íƒìƒ‰í•˜ëŠ”ê²Œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤**.

ì´ë•Œì˜ ë²”ìœ„ëŠ” ëŒ€ëµì ìœ¼ë¡œ ì§€ì •í•˜ëŠ”ê²ƒì´ íš¨ê³¼ì ì¸ë°
**_0.001 ~1,000 ì‚¬ì´ì™€ ê°™ì´ 10ì˜ ê±°ë“­ì œê³± ë‹¨ìœ„ë¡œ ë²”ìœ„ë¥¼ ì§€ì •_**í•œë‹¤.
ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì§€ì •í•˜ëŠ” ê²ƒì„ **_ë¡œê·¸ìŠ¤ì¼€ì¼ë¡œ ì§€ì •í•œë‹¤_**ê³  í•œë‹¤.

**_í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™”ì—ëŠ” ì˜¤ëœì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆë‹¤_**.
ë”°ë¼ì„œ ì•ˆ ì¢‹ì„ ê²ƒ ê°™ì€ ê°’ì€ ì¼ì° í¬ê¸°í•˜ê³ , **epoch ê°’ì„ ì‘ê²Œí•˜ì—¬** 1íšŒ í‰ê°€ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•˜ì

1. **_í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì˜ ë²”ìœ„ë¥¼ ì„¤ì •_**í•œë‹¤
2. ì„¤ì •ëœ ë²”ìœ„ì—ì„œ **_í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œ_**
3. 1ë‹¨ê³„ì—ì„œ **_ìƒ˜í”Œë§í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì‚¬ìš©í•´ í•™ìŠµí•˜ê³ , ê²€ì¦ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€í•œë‹¤(epoch ë‹¨ìœ„ì‘ê²Œì„¤ì •)_**
4. **1ë‹¨ê³„ì™€ 2ë‹¨ê³„ë¥¼ íŠ¹ì •íšŸìˆ˜ ë°˜ë³µí•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ ì¢í˜€ê°„ë‹¤**

> ì¡°ê¸ˆë” ì„¸ë ¨ëœ ê¸°ë²•ìœ¼ë¡œëŠ” `ë² ì´ì¦ˆ ìµœì í™” ê¸°ë²•`ì´ìˆë‹¤.(ë² ì´ì¦ˆ ì´ë¡ ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì—¬ ë” ì—„ë°€í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤.)

```python
weight_decay = 10 ** np.random.uniform(-8,-4)
lr = 10** np.random.uniform(-6,-2)
```

ìœ„ì™€ê°™ì´ ì„¤ì •í•˜ì—¬ **_ë°˜ë³µí•˜ë‹¤ê°€ ê²€ì¦ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„, í•™ìŠµì´ ì˜ ì§„í–‰ë˜ê³ ìˆëŠ” ë¶€ë¶„ê¹Œì§€ íŒŒì•…í•˜ì—¬ ê·¸ ë¶€ê·¼ ë²”ìœ„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµí•˜ê¸° ì‹œì‘_**í•œë‹¤.

#### ğŸ‘¨â€ğŸ’»ì‹¤ìŠµì½”ë“œ

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[keys] -= self.lr = grads[key]


class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def updata(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

# ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ vëŠ” ë¬¼ì²´ì˜ ì†ë„ì´ë‹¤.
# update()ê°€ í˜¸ì¶œë ë•Œ ë§¤ê°œë³€ìˆ˜ì™€ ê°™ì€ êµ¬ì¡°ì˜ dataë¥¼ ë”•ì…”ë„ˆë¦¬ ë³€ìˆ˜ë¡œ ì €ì¥


class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[ey]) + 1e-7)

    # ë§ˆì§€ë§‰ì— 1e-7ì„ ë”í•˜ëŠ” ë¶€ë¶„ì€ self.h[key]ê°€ 0ì´ë¼í•´ë„ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‚¬íƒœë¥¼ ë§‰ëŠ”ë‹¤.

```
