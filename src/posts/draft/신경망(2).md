---
title: '신경망 학습'
date: '2020-06-18'
category: []
draft: True
---

> 학습 : 훈련데이터로부터 **_가중치 매개변수의 최적값_**을 자동으로 획득하는 것, 신경망에서는 **_cost function_**을 기반으로 한다.

실제 신경망의 매개변수는 정말 많은데 이를 수작업으로 설정하는것은 불가능하다.
따라서 신경망 학습을 통해서 재설정을 해주게 된다.
(퍼셉트론으로도 선형분리문제는 학습으로 풀수있지만 비선형 분리문제는 학습할수 없다.)

우리는 DATA를 보고 직관과 경험을 통해서 규칙성을 발견할수 있는 반면 프로그램은 `feature`를 추출하고 `pattern`을 ML로 학습하는 방법이있다.

이 때의 `feature`은 input data에서 중요한 data를 추출할수 있게 설계된 변환기인데 **img의 경우 verctor로 기술하고 computer vision의 경우 SIFT, SURF, HOG**등의 특징을 많이사용한다.
이렇게 **data를 vector로 변환하고 vector를 지도학습의 분류방법인 SVM,KNN**등으로 학습한다.

> 이때 규칙을 찾아내는 역할은 기계가 하는데 이러한 **img를 vector로 변환하는 과정은 사람이 하므로**, 적절한 feature를 뽑아내는 것도 중요하다.

즉 기계가 학습을 하는 패러다임이 크게 3가지가 있다.

-   사람이 생각한 알고리즘
-   사람이 생각한 `feature(SIFT,HOG)`로 `기계학습(SVM,KNN)`
-   신경망(딥러닝) - **_중요한 특징까지도 기계가 스스로 학습함_**, **_사람의 개입이 없다._**

학습문제는 DATA를 `Training DATA`와`TEST DATA`로 나눠 학습과 실험을 진행한다. 그 이유는 **_범용적으로 사용할수 있는 모델을 개발_**하기 위해서 데이터를 분리한다.

DATASET 하나로만 학습과 평가를 하면 신뢰성있는 결과를 얻기 힘들다.
이렇게 하나의 DATASET에 지나치게 최적화 된 상태를 `overfitting`이라고 한다.

신경망에서는 최적의 매개변수를 찾기위해 `cost function`을 지표로 사용한다.
그리고 신경망에서는 일반적으로 `오차제곱의 합(Sum of squares for Error, SSE)`과 `교차엔트로피오차(Cross entropy error, CEE)`를 사용한다.

-   **_오차제곱합(Sum of squares for Error, SSE)_**  
     이때의 오차는 신경망의 출력(y)와 정답 레이블(t)간의 Error를 말한다.

> `one-hot encoding` : 정답이 2일때 `[0,0,1,0,0]` 이런식으로 정답에 해당하는 `index`만 1로 표현하는 표기법

-   **_교차 엔트로피 오차(Cross entropy error, CEE)_**
    (y:신경망의 출력, t:정답레이블)  
    `수식 : E = -\sum_{k=1}(t*k\*log(y_k))`
    위의 식은 실질적인 정답일때의 추정값의 자연로그를 계산하는 식이 된다.
    (왜냐면 정답값은 **one-hot encoding으로 1이되고 그때의 추정값은 확률로 나타니기 때문이다.**)
    즉 교차엔트로피오차는 정답일때의 **_출력이 값을 결정하는데 중요한 요소_**가 된다.

#### 미니배치학습

기계학습 문제는 방대한 훈련 data에서 모두 일일이 cost function을 구하려면 비효율적일 수 있다(왜냐하면 N만큼 확장을 하고 N으로 나누어 정규화 시켜줘야하기때문에) 따라서 신경망에서도 훈련 data로 부터 **_일부만 골라서 학습_**을 수행하게 되는데 이 일부를 `mini-batch`라고 한다.
6만개의 data가 있으면 그중 100장을 뽑아 100장만을 사용하여 학습하는 것을 말한다. (표본을 뽑아 모집단을 대표하기)

ex) mnist 를 이용해서 실습을 할때 `np.random.choice()`를 사용해서 무작위로 batch_size만큼 뽑아오자
`np.random.choice(60000,100)`

### 왜 손실 함수를 설정할까

정확도를 높이기 위해 cost function을 설정하는 이유는 미분(기울기,변화율)를 통해서 cost function을 작게하는 값을 찾아갈 수 있기 때문이다.
이때의 미분값은 **_가중치의 매개변수를 변화시켰을때 손실함수가 어떻게 변하냐_**를 나타내게 되고,
미분값이 0이 매개변수의 갱신이 멈춘다.
(따라서 신경망학습에서는 기울기가 0이 되지않는것이 중요하다).

> 정확도를 지표로 삼아버리면 미분값이 대부분의 장소에서 0이 되어 갱신이 멈출 수 있다. 왜냐하면 매개변수를 조금 옮기는 것만으로는 정확도의 개선이 이루어 지기 어렵고, 정확도는 상대적으로 불연속적인 모양을 나타내기 때문이다.  
> 이러한 이유때문에 계단함수도 활성화 함수로 사용하지 않는다.(매개변수의 변화에따라 손실함수의 변화를 주지 않아 학습이 원할이 이뤄지지 않기때문)

즉 경사법에서는 **미분이 학습방향결정의 중요한 역할**을 한다.

### 미분

파이썬으로 미분 공식을 나타낸것, h는 작은 값을 임의로 대입해준다.

```python
def numerical_diff(f, x): # 이름은 수치미분으로 해준것.
    h=10e - 50
    return (f(x+h)-f(x))/h
```

그런데 위에서 넣어준 임의의 h 값 `10e-50` 은 가수가 10이므로 소숫점 49자리 숫자인데 이 숫자는 `rounding error`문제를 일으킨다
(소수점 몇자리 이하 반올림으로 계산결과에 오차가 생기는것)

`print(np.float32(1e-50))`
따라서 적당히 `10^-4`정도만 해주도록 하자

이로써 발생하는 새로운 문제점이있는데 h를 무한히 0으로 좁힐수 없어서
`점과 점+증분 사이의 변화율`이 아니라 지금은 그저 `점과 점사이의 변화율`로 나타나는것이 문제다(어느 순간의 순간 기울기가 아니라 점과 점사이의 기울기라는 말)

따라서 이부분은 x+h뿐만 아니라 x-h도 생각을 해주어 차분을 계산해주는데
이를 `중심차분` 혹은 `중앙차분`이라고 해준다. (x와 x+h간에는 전방차분값)

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h)- f(x-h)/(2*h)) #중심차분 적용
```

-   이와같이 아주 작은 차분으로 미분하는 것을 수치미분(근사치)이라 하고,

*   수식을 전개해 미분하는 것을 해석적으로 미분한다고 한다.(이는 진정한 미분값을 구해준다.)

> 따라서 수치해석학은 해석학 문제에서 수치적 근삿값을 구하는 알고리즘을 연구하는 수학이다.

###### 여기서 뜬금없지만 python은 한번 컴파일 되고 실행되니까 컴파일 될때 에러가 있는지 체크를 해주고 그다음 실행이 되네

### 편미분

변수가 2개이상일때 어느 변수에 대한 미분인가를 구별해서 해주게 되는데 이를 편미분이라 한다. 이때 다른 애들을 상수 취급해주고 수치미분을 적용해주자. 이때의 미분값도 특정 장소의 기울기가 되는데 어떤 변수에 포커싱했다는 것만 다를 뿐이다.

그런데 이때의 편미분을 각 변수별로 따로하는게 아니라 모든 변수의 편미분을 벡터로 정리하면 **_기울기_**가 된다.

편미분또한 **_미분과 같이 각점에서의 기울기를 구해줄수있고 이러한 기울기, 벡터들로는 벡터장_**을 그려줄수있다.
그림 그려진 벡터장으로 확인읋 해보면 기울기(화살표)는 각 지점에서 **_낮아지는 방향_**으로 가리키는데 이는 해당 방향으로 갈때 함수의 출력값이 줄어드는 것을 의미한다.

### 경사법

신경망은 cost function을 적게하는 **_매개변수(가중치와 편향)_**를 학습시에 찾아줘야하는데 실제로 구성되는 손실함수도 복잡하고 매개변수 공간 또한 넓어 최솟값을 찾아가기 힘들다. 이러한 최솟값을 찾아가는게 경사법이다.
하지만 복잡한 함수에서는 기울기가 가리키는 곳으로 간다해도 **_극솟값, 최솟값, 안장점_**으로 인도할수 있고 이때 극솟값과 안장점에서는 최솟값이라고 부르기 힘들다.

하지만 그럼에도 **_cost 를 줄일수 있는 방향으로 나아가야 하므로 기울기에 기반한 경사법으로 방향을 설정한다_**.(신경망 학습에서 일반적으로 많이 사용한다)

### 학습률(learning rate)

학습률이란 한번의 학습으로 얼마만큼 반영하는지 결정하는 것으로 너무 크거나 작으면 최적의 값을 찾아가기 힘들다. 이 값을 변경시키면서 올바르게 학습이 진행되는지 체크한다.

```python

def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x

init_x = np.array([-3.0, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))

#처음 x값을 3,4로 설정했는데 0.0에 정말 근사한 값으로 다가가는 모습을 확인할수 있다
```

-   학습률이 너무 크면 튕겨져 나가거나
-   학습률이 너무 작으면 학습이 제대로 이루어 지지 않아

`learning_rate` 의 설정도 중요하다. 학습률과 같은 매개변수를 하이퍼파라미터라 하고, 하이퍼파라미터를 엔지니어가 직접 설정하여 가장 최적의 값을 찾는 과정도 중요하다.

#### 신경망에서의 기울기

신경망 또한 주어진 가중치(W)에 대해서 손실함수(L)의 기울기를 나타내고 이를 통해서 학습할 수 있다.

각 W의 값에대한 기울기가 나오게 되고 W를 어떻게 조작하는가에 따라서 손실함수의 값도 달라질수 있으므로 우리가 원하는 손실함수를 최소화 하는 지점으로 간다.

### 학습 알고리즘 구현하기

[신경망 학습의 절차]

> 신경망에는 가중치와 편향이있고 trainset에 적용하도록 조정하는 과정을 train이라한다.

1. `mini-batch` : train-data중 표본을 뽑아서 학습한다.
2. `gradient` : 배치 값의 cost function을 줄이기 위해 gradient를 구하고 이는 학습 방향을 제시한다.
3. **매개변수 갱신** : W를 gradient방향으로 조금 갱신
4. 1~3 반복

위와같이 미니배치로 하강하는 방법을 확률적경사하강 `stochastic gradient descent(SGD)` 라한다.

> `Epoch` 은 하나의 단위, 1Epoch은 학습에서 훈련데이터를 모두 훈련했을경우, 예를 들면 10000개의 훈련 data가 잇을때 batch가 100개라면 **_100회 훈련한것이 1epoch이 된다._**
> 따라서 정확도같은경우 **1epoch**당 표기을 해주면 좀더 큰 관점에서 볼수 있다.

---

### 공부하는데 사용한 코드

```python
from gradient import numerical_gradient as real_num_grad
from functions import softmax, cross_entropy_error as real_cross_entropy
import matplotlib.pylab as plt
from mnist import load_mnist
import numpy as np
import sys
import os
sys.path.append(os.pardir)
# 오차제곱합 cost function


def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)


t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
result = sum_squares_error(np.array(y), np.array(t))
print(result)
result2 = sum_squares_error(np.array(y2), np.array(t))
print(result2)


def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta))


a = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
b = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
b2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
result3 = cross_entropy_error(np.array(b), np.array(a))
print(result3)
result4 = cross_entropy_error(np.array(b2), np.array(a))
print(result4)
# log를 계산할때 작은 delta값을 더해줬다. 이는 np.log() 함수에 0이 들어가면 계산을 진행할수없기에 아주 작은 값이라도 더해
# 0이 안되게 해주는것이다.


(x_train, t_train), (x_test, t_test) =\
    load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]


def cross_entropy_error2(y, t):
    if y.ndim == 1:  # 데이터가 1개일경우
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+1e-7))/batch_size
# y가 신경망 출력, t가 정답레이블 y가 1차원이면, 즉 데이터 1개당 크로스 엔트로필를 구하는 경우
# reshape로 data형상을 바꿔준다


def cross_entropy_error3_notonehot(y, t):
    if y.ndim == 1:  # 데이터가 1개일경우
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t]+1e-7))/batch_size
# 여기서 핵심은 t가 0인 원소는 크로스 엔트로피도 0이므로 계산은 무시해도 된다.
# 다시말해서 정답에 해당하는 출력이랑만 비교하자

# np.log(y[np.arange(batch_size),t])란 무엇인가요
# np.arange는 0부터 batch_size전까지 배열을 생성한다.
# y[np.arange,t] 는 각 데이터의 정답 레이블에 해당하는 신경망 출력 추출
# 그러면 y[0,2], y[1,7] 이런식으로 배열이 생성됨


# 파이썬으로 미분 공식을 나타낸것, h는 작은 값을 임의로 대입해준다.
    # 이름은 수치미분으로 해준것.
def numerical_diff(f, x):
    h = 10*e - 50
    return (f(x+h)-f(x))/h

# 그런데 위에서 넣어준 임의의 h 값 10e-50 은 가수가 10이므로 소숫점 49자리 숫자인데
# 이 숫자는 rounding error문제를 일으킨다(소수점 몇자리 이하 반올림으로 계산결과에 오차가 생기는것,)


print(np.float32(1e-50))

# 따라서 적당히 10^-4정도만 해주도록 하자

# h의 값과 차분을 무한히 줄일수 없어서 나타나는 에러를 수정하여 다시 작성해보면


def numerical_diff2(f, x):
    h = 1e-4  # 0.0001
    return (f(x+h) - f(x-h))/(2*h)


# 이를 바탕으로 실습을 한번해보자

def function_1(x):
    return 0.01*x**2 + 0.1*x


# x = np.arange(0.0, 20.0, 0.1)
# y = function_1(x)
# plt.xlabel("x")
# plt.ylabel("f(x)")
# plt.plot(x, y)
# plt.show()


print(numerical_diff2(function_1, 5))

print(numerical_diff2(function_1, 10))
# 위 두값의 실제 값은 각각 0.2와 0.3인데 수치미분과 비교하면 오차가 매우 작음을 알수있따.


# 편미분
def function_2(x):
    return x[0]**2 + x[1] ** 2
    # 혹은 return np.sum(x**2)


def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)  # x와 같은 크기만큼 0로 배열 생성

    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)

        # f(x+h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val

    return grad


print(numerical_gradient(function_2, np.array([3.0, 4.0])))
print(numerical_gradient(function_2, np.array([0.0, 2.0])))
print(numerical_gradient(function_2, np.array([3.0, 0.0])))

# 이렇게 해주면 각 점에서의 기울기를 계산해줄수 잇다.
# 그리고 위에서 구한 기울기, 벡터들로는 벡터장을 그려줄수있다.
# 그림 그려진 벡터장으로 확인읋 해보면 기울기(화살표)는 각 지점에서 낮아지는 방향을 가리키는데
# 이는 해당 방향으로 갈때 함수의 출력값이 줄어드는 것을 의미한다.


def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
    return x


init_x = np.array([-3.0, 4.0])
print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))
print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))
print("?", gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))

# 처음 x값을 3,4로 설정했는데 0.0에 정말 근사한 값으로 다가가는 모습을 확인할수 있따


class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3)  # 정규분포로 초기화

    # 예측하는 함수
    def predict(self, x):
        return np.dot(x, self.W)

    # 손실함수
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = real_cross_entropy(y, t)
        return loss


net = simpleNet()
print(net.W)

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
print(np.argmax(p))

t = np.array([0, 0, 1])
print(net.loss(x, t))

# 기울기 구하기


def f(W):  # 직접적으로 W를 받고있지는 않지만 같은 instance의 W를 사용해서 내부적으로 메서드작업을 해주고있따.
    return net.loss(x, t)


dW = real_num_grad(f, net.W)
print(dW)

# 이러한 기울기를 보면 각 W에서의 중요도 및 양으로 움직일때 손실함수가 증가하는지 감소하는지 확인 할 수 있다.

# def f(W):  # 직접적으로 W를 받고있지는 않지만 같은 instance의 W를 사용해서 내부적으로 메서드작업을 해주고있다..
#     return net.loss(x, t)
# #위의 함수는 아래와 같이 람다함수로도 구현한다.
# f = lambda w: net.loss(x,t)

```
